{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell-pc\\Anaconda3\\lib\\site-packages\\deap\\tools\\_hypervolume\\pyhv.py:33: ImportWarning: Falling back to the python version of hypervolume module. Expect this to be very slow.\n",
      "  \"module. Expect this to be very slow.\", ImportWarning)\n",
      "C:\\Users\\Dell-pc\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>google score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.20000000298023224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.10000000149011612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           google score\n",
       "0                   0.0\n",
       "1                   0.0\n",
       "2  -0.20000000298023224\n",
       "3                   0.0\n",
       "4  -0.10000000149011612"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google = []\n",
    "with open('normalized_google.csv') as f :\n",
    "    reader = csv.reader(f)\n",
    "    for rows in reader:\n",
    "        google.append(rows)\n",
    "\n",
    "google = google[1:100]\n",
    "google = pd.DataFrame(google,columns=['id','google score','normalized'])\n",
    "\n",
    "google_ml = google[['google score']]\n",
    "\n",
    "google_ml.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aws_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  aws_score\n",
       "0      0.99\n",
       "1      0.88\n",
       "2      0.85\n",
       "3      0.87\n",
       "4      0.96"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aws = []\n",
    "with open('aws_score.csv') as f :\n",
    "    reader = csv.reader(f)\n",
    "    for rows in reader:\n",
    "        aws.append(rows)\n",
    "\n",
    "aws = aws[1:100]\n",
    "aws = pd.DataFrame(aws,columns=['id','aws_score',])\n",
    "\n",
    "aws_ml = aws[['aws_score']]\n",
    "\n",
    "aws_ml.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>watson_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  watson_score\n",
       "0          0.0\n",
       "1         0.77\n",
       "2         0.71\n",
       "3          0.0\n",
       "4         0.76"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "watson = []\n",
    "with open('watson_score.csv') as f :\n",
    "    reader = csv.reader(f)\n",
    "    for rows in reader:\n",
    "        watson.append(rows)\n",
    "\n",
    "watson = watson[1:100]\n",
    "watson = pd.DataFrame(watson,columns=['id','watson_score'])\n",
    "\n",
    "watson_ml = watson[['watson_score']]\n",
    "\n",
    "watson_ml.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>azure score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  azure score\n",
       "0        0.5 \n",
       "1        0.5 \n",
       "2        0.5 \n",
       "3        0.5 \n",
       "4        0.89"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azure = []\n",
    "with open('normalized_azure.csv') as f :\n",
    "    reader = csv.reader(f)\n",
    "    for rows in reader:\n",
    "        azure.append(rows)\n",
    "\n",
    "azure = azure[1:100]\n",
    "azure = pd.DataFrame(azure,columns=['id','azure score','normalized'])\n",
    "\n",
    "azure_ml = azure[['azure score']]\n",
    "\n",
    "azure_ml.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manual score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  manual score\n",
       "0          0.5\n",
       "1            1\n",
       "2          0.5\n",
       "3          0.5\n",
       "4            1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual = []\n",
    "with open('manual_sentiment.csv') as f :\n",
    "    reader = csv.reader(f)\n",
    "    for rows in reader:\n",
    "        manual.append(rows)\n",
    "\n",
    "manual = manual[1:100]\n",
    "manual = pd.DataFrame(manual,columns=['id','manual score'])\n",
    "\n",
    "manual_ml = manual[['manual score']]\n",
    "manual_ml = manual_ml.replace('0','0.5')\n",
    "manual_ml = manual_ml.replace('-1','0')\n",
    "manual_ml.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPOT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TPOT with AMAZON \n",
    "ms=[]\n",
    "with open('manual_sentiment.csv') as f :\n",
    "    reader = csv.reader(f)\n",
    "    for rows in reader:\n",
    "        ms.append(rows)\n",
    "        \n",
    "q = []\n",
    "for i in range(0,98):\n",
    "    m = ms[1:100][i][1]\n",
    "    q.append(m)\n",
    "\n",
    "q = pd.DataFrame(q,columns=['manual'])\n",
    "\n",
    "aws=[]\n",
    "with open('aws_score.csv') as f :\n",
    "    reader = csv.reader(f)\n",
    "    for rows in reader:\n",
    "        aws.append(rows)\n",
    "\n",
    "e = []\n",
    "for i in range(0,98):\n",
    "    m = aws[1:100][i][1]\n",
    "    e.append(m)\n",
    "\n",
    "e = pd.DataFrame(e,columns=['aws'])\n",
    "\n",
    "azure=[]\n",
    "with open('azure_score.csv') as f :\n",
    "    reader = csv.reader(f)\n",
    "    for rows in reader:\n",
    "        azure.append(rows)\n",
    "\n",
    "o = []\n",
    "for i in range(0,98):\n",
    "    m = azure[1:100][i][1]\n",
    "    o.append(m)\n",
    "\n",
    "s = pd.DataFrame(o,columns=['azure'])\n",
    "\n",
    "google=[]\n",
    "with open('google_score.csv') as f :\n",
    "    reader = csv.reader(f)\n",
    "    for rows in reader:\n",
    "        google.append(rows)\n",
    "\n",
    "p = []\n",
    "for i in range(0,98):\n",
    "    m = google[1:100][i][1]\n",
    "    p.append(m)\n",
    "\n",
    "g = pd.DataFrame(p,columns=['google'])\n",
    "\n",
    "watson=[]\n",
    "with open('watson_score.csv') as f :\n",
    "    reader = csv.reader(f)\n",
    "    for rows in reader:\n",
    "        watson.append(rows)\n",
    "\n",
    "a = []\n",
    "for i in range(0,98):\n",
    "    m = watson[1:100][i][1]\n",
    "    a.append(m)\n",
    "\n",
    "h = pd.DataFrame(a,columns=['watson'])\n",
    "\n",
    "\n",
    "d = e.join(s)\n",
    "d = d.join(g)\n",
    "d = d.join(h)\n",
    "#lbl = LabelEncoder()\n",
    "#y = lbl.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lbl = LabelEncoder()\n",
    "q = lbl.fit_transform(q)\n",
    "d = pd.get_dummies(d).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(d,q,test_size=0.3)\n",
    "#X_train =lbl.fit_transform(X_train)\n",
    "#X_test =lbl.fit_transform(X_test)\n",
    "#X_test = pd.get_dummies(X_test).values\n",
    "#X_train = pd.get_dummies(X_train).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n",
      "29 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63866c35399a42e68ef70a66863ac274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.6476190476190476\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.6476190476190476\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.6476190476190476\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.6476190476190476\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.6476190476190476\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 6 - Current Pareto front scores:\n",
      "-1\t0.6476190476190476\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 7 - Current Pareto front scores:\n",
      "-1\t0.6476190476190476\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Generation 8 - Current Pareto front scores:\n",
      "-1\t0.6476190476190476\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 9 - Current Pareto front scores:\n",
      "-1\t0.6476190476190476\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 10 - Current Pareto front scores:\n",
      "-1\t0.6476190476190476\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 11 - Current Pareto front scores:\n",
      "-1\t0.6476190476190476\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 12 - Current Pareto front scores:\n",
      "-1\t0.6476190476190476\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 13 - Current Pareto front scores:\n",
      "-1\t0.6476190476190476\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 14 - Current Pareto front scores:\n",
      "-1\t0.6476190476190476\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 15 - Current Pareto front scores:\n",
      "-1\t0.6476190476190476\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Generation 16 - Current Pareto front scores:\n",
      "-1\t0.6476190476190476\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 17 - Current Pareto front scores:\n",
      "-1\t0.6476190476190476\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 18 - Current Pareto front scores:\n",
      "-1\t0.65\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=9, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 19 - Current Pareto front scores:\n",
      "-1\t0.65\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=9, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 20 - Current Pareto front scores:\n",
      "-1\t0.65\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=9, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "Generation 21 - Current Pareto front scores:\n",
      "-1\t0.65\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=9, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 22 - Current Pareto front scores:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\t0.65\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=9, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 23 - Current Pareto front scores:\n",
      "-1\t0.65\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=9, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 24 - Current Pareto front scores:\n",
      "-1\t0.680952380952381\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 25 - Current Pareto front scores:\n",
      "-1\t0.680952380952381\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 26 - Current Pareto front scores:\n",
      "-1\t0.680952380952381\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 27 - Current Pareto front scores:\n",
      "-1\t0.680952380952381\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-3\t0.6928571428571428\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "Generation 28 - Current Pareto front scores:\n",
      "-1\t0.680952380952381\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-3\t0.6952380952380952\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 29 - Current Pareto front scores:\n",
      "-1\t0.680952380952381\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-3\t0.6952380952380952\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 30 - Current Pareto front scores:\n",
      "-1\t0.680952380952381\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-3\t0.6952380952380952\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-4\t0.7095238095238096\tGradientBoostingClassifier(PCA(VarianceThreshold(StandardScaler(input_matrix), VarianceThreshold__threshold=0.0005), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 31 - Current Pareto front scores:\n",
      "-1\t0.680952380952381\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-3\t0.6952380952380952\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-4\t0.7095238095238096\tGradientBoostingClassifier(PCA(VarianceThreshold(StandardScaler(input_matrix), VarianceThreshold__threshold=0.0005), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 32 - Current Pareto front scores:\n",
      "-1\t0.680952380952381\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 33 - Current Pareto front scores:\n",
      "-1\t0.680952380952381\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6952380952380952\tGradientBoostingClassifier(PCA(CombineDFs(input_matrix, input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=12, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 34 - Current Pareto front scores:\n",
      "-1\t0.680952380952381\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6952380952380952\tGradientBoostingClassifier(PCA(CombineDFs(input_matrix, input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=12, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "Generation 35 - Current Pareto front scores:\n",
      "-1\t0.680952380952381\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6952380952380952\tGradientBoostingClassifier(PCA(CombineDFs(input_matrix, input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=12, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 36 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 37 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 The condensed distance matrix must contain only finite values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 38 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 39 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 40 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 41 - Current Pareto front scores:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "Generation 42 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 43 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-5\t0.7238095238095238\tGradientBoostingClassifier(RFE(PCA(StandardScaler(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), PCA__iterated_power=2, PCA__svd_solver=randomized), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.9500000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.55), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=15, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Generation 44 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-5\t0.7238095238095238\tGradientBoostingClassifier(RFE(PCA(StandardScaler(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), PCA__iterated_power=2, PCA__svd_solver=randomized), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.9500000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.55), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=15, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "Generation 45 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-5\t0.7238095238095238\tGradientBoostingClassifier(RFE(PCA(StandardScaler(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), PCA__iterated_power=2, PCA__svd_solver=randomized), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.9500000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.55), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=15, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 46 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-5\t0.7238095238095238\tGradientBoostingClassifier(RFE(PCA(StandardScaler(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.1, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10)), PCA__iterated_power=2, PCA__svd_solver=randomized), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.9500000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.55), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=15, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 47 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-4\t0.7238095238095238\tGradientBoostingClassifier(PCA(DecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=2, DecisionTreeClassifier__min_samples_leaf=13, DecisionTreeClassifier__min_samples_split=6), PCA__iterated_power=7, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 48 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-4\t0.7238095238095238\tGradientBoostingClassifier(PCA(DecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=2, DecisionTreeClassifier__min_samples_leaf=13, DecisionTreeClassifier__min_samples_split=6), PCA__iterated_power=7, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "Generation 49 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-4\t0.7238095238095238\tGradientBoostingClassifier(PCA(DecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=2, DecisionTreeClassifier__min_samples_leaf=13, DecisionTreeClassifier__min_samples_split=6), PCA__iterated_power=7, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 50 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-4\t0.7238095238095238\tGradientBoostingClassifier(PCA(DecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=2, DecisionTreeClassifier__min_samples_leaf=13, DecisionTreeClassifier__min_samples_split=6), PCA__iterated_power=7, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 51 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-4\t0.7238095238095238\tGradientBoostingClassifier(PCA(DecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=2, DecisionTreeClassifier__min_samples_leaf=13, DecisionTreeClassifier__min_samples_split=6), PCA__iterated_power=7, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55\n",
      "Generation 52 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4\t0.7238095238095238\tGradientBoostingClassifier(PCA(DecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=2, DecisionTreeClassifier__min_samples_leaf=13, DecisionTreeClassifier__min_samples_split=6), PCA__iterated_power=7, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Skipped pipeline #5345 due to time out. Continuing to the next pipeline.\n",
      "Generation 53 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-4\t0.7238095238095238\tGradientBoostingClassifier(PCA(DecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=2, DecisionTreeClassifier__min_samples_leaf=13, DecisionTreeClassifier__min_samples_split=6), PCA__iterated_power=7, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 54 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-4\t0.7238095238095238\tGradientBoostingClassifier(PCA(DecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=2, DecisionTreeClassifier__min_samples_leaf=13, DecisionTreeClassifier__min_samples_split=6), PCA__iterated_power=7, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Skipped pipeline #5537 due to time out. Continuing to the next pipeline.\n",
      "Generation 55 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7095238095238096\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=8, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-4\t0.7238095238095238\tGradientBoostingClassifier(PCA(DecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=2, DecisionTreeClassifier__min_samples_leaf=13, DecisionTreeClassifier__min_samples_split=6), PCA__iterated_power=7, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Skipped pipeline #5625 due to time out. Continuing to the next pipeline.\n",
      "Generation 56 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "Generation 57 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "Generation 58 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Skipped pipeline #5983 due to time out. Continuing to the next pipeline.\n",
      "Generation 59 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 60 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 61 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Skipped pipeline #6249 due to time out. Continuing to the next pipeline.\n",
      "Generation 62 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 63 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "Generation 64 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "Generation 65 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Skipped pipeline #6621 due to time out. Continuing to the next pipeline.\n",
      "Generation 66 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 67 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 68 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 69 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 70 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 71 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 72 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 73 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 74 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 75 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "Generation 76 - Current Pareto front scores:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 77 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 78 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 79 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 80 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 81 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 82 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Skipped pipeline #8315 due to time out. Continuing to the next pipeline.\n",
      "Generation 83 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 84 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 85 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 86 - Current Pareto front scores:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 87 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 88 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 89 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 90 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 91 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 92 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 93 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 94 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 95 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 96 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 97 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 98 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 99 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 100 - Current Pareto front scores:\n",
      "-1\t0.6952380952380952\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "-2\t0.6976190476190476\tRandomForestClassifier(LogisticRegression(input_matrix, LogisticRegression__C=25.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.7238095238095238\tGradientBoostingClassifier(PCA(StandardScaler(input_matrix), PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(config_dict=None, crossover_rate=0.1, cv=5,\n",
       "        disable_update_check=False, early_stop=None, generations=100,\n",
       "        max_eval_time_mins=5, max_time_mins=None, memory=None,\n",
       "        mutation_rate=0.9, n_jobs=1, offspring_size=None,\n",
       "        periodic_checkpoint_folder=None, population_size=100,\n",
       "        random_state=None, scoring='accuracy', subsample=1.0,\n",
       "        use_dask=False, verbosity=3, warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot = TPOTClassifier(verbosity=3, \n",
    "                      scoring=\"accuracy\"\n",
    "                      )\n",
    "\n",
    "tpot.fit(X_train,y_train)\n",
    "#print(tpot.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  2],\n",
       "       [ 0,  3,  6],\n",
       "       [ 0,  3, 14]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "predict = tpot.predict(X_test)\n",
    "cm = confusion_matrix(y_test,predict)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H2O.AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h2o\n",
      "  Downloading https://files.pythonhosted.org/packages/58/c2/be92023ecfab14f39c838e5dfe0c566519d03bc2dbedf3df27316456d2e1/h2o-3.22.0.1.tar.gz (120.6MB)\n",
      "Requirement already satisfied: requests in c:\\users\\dell-pc\\anaconda3\\lib\\site-packages (from h2o)\n",
      "Collecting tabulate (from h2o)\n",
      "  Downloading https://files.pythonhosted.org/packages/12/c2/11d6845db5edf1295bc08b2f488cf5937806586afe42936c3f34c097ebdc/tabulate-0.8.2.tar.gz (45kB)\n",
      "Requirement already satisfied: future in c:\\users\\dell-pc\\anaconda3\\lib\\site-packages (from h2o)\n",
      "Requirement already satisfied: colorama>=0.3.8 in c:\\users\\dell-pc\\anaconda3\\lib\\site-packages (from h2o)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell-pc\\anaconda3\\lib\\site-packages (from requests->h2o)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\dell-pc\\anaconda3\\lib\\site-packages (from requests->h2o)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\dell-pc\\anaconda3\\lib\\site-packages (from requests->h2o)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in c:\\users\\dell-pc\\anaconda3\\lib\\site-packages (from requests->h2o)\n",
      "Building wheels for collected packages: h2o, tabulate\n",
      "  Running setup.py bdist_wheel for h2o: started\n",
      "  Running setup.py bdist_wheel for h2o: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Dell-pc\\AppData\\Local\\pip\\Cache\\wheels\\82\\79\\1a\\0a69bcc2bdcb643b310ef61cc1a4d2ca3abe03059712cc804e\n",
      "  Running setup.py bdist_wheel for tabulate: started\n",
      "  Running setup.py bdist_wheel for tabulate: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Dell-pc\\AppData\\Local\\pip\\Cache\\wheels\\2a\\85\\33\\2f6da85d5f10614cbe5a625eab3b3aebfdf43e7b857f25f829\n",
      "Successfully built h2o tabulate\n",
      "Installing collected packages: tabulate, h2o\n",
      "Successfully installed h2o-3.22.0.1 tabulate-0.8.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; OpenJDK 64-Bit Server VM (Zulu 8.20.0.5-win64) (build 25.121-b15, mixed mode)\n",
      "  Starting server from C:\\Users\\Dell-pc\\Anaconda3\\lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\Dell-pc\\AppData\\Local\\Temp\\tmpq0sz4gi4\n",
      "  JVM stdout: C:\\Users\\Dell-pc\\AppData\\Local\\Temp\\tmpq0sz4gi4\\h2o_Dell_pc_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\Dell-pc\\AppData\\Local\\Temp\\tmpq0sz4gi4\\h2o_Dell_pc_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>23 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>America/New_York</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.22.0.1</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>27 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_Dell_pc_fy07rn</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>1.754 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.3 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  -------------------------------\n",
       "H2O cluster uptime:         23 secs\n",
       "H2O cluster timezone:       America/New_York\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.22.0.1\n",
       "H2O cluster version age:    27 days\n",
       "H2O cluster name:           H2O_from_python_Dell_pc_fy07rn\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    1.754 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.3 final\n",
       "--------------------------  -------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#aws_ml.to_csv('aws_ml.csv')\n",
    "#manual_ml.to_csv('manual_score.csv')\n",
    "azure_ml.to_csv('azure_score.csv')\n",
    "google_ml.to_csv('google_score.csv')\n",
    "watson_ml.to_csv('watson_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ms=[]\n",
    "with open('manual_sentiment.csv') as f :\n",
    "    reader = csv.reader(f)\n",
    "    for rows in reader:\n",
    "        ms.append(rows)\n",
    "        \n",
    "q = []\n",
    "for i in range(0,98):\n",
    "    m = ms[1:100][i][1]\n",
    "    q.append(m)\n",
    "\n",
    "q = pd.DataFrame(q,columns=['manual'])\n",
    "\n",
    "aws=[]\n",
    "with open('aws_score.csv') as f :\n",
    "    reader = csv.reader(f)\n",
    "    for rows in reader:\n",
    "        aws.append(rows)\n",
    "\n",
    "e = []\n",
    "for i in range(0,98):\n",
    "    m = aws[1:100][i][1]\n",
    "    e.append(m)\n",
    "\n",
    "\n",
    "\n",
    "e = pd.DataFrame(e,columns=['aws'])\n",
    "\n",
    "azure=[]\n",
    "with open('azure_score.csv') as f :\n",
    "    reader = csv.reader(f)\n",
    "    for rows in reader:\n",
    "        azure.append(rows)\n",
    "\n",
    "o = []\n",
    "for i in range(0,98):\n",
    "    m = azure[1:100][i][1]\n",
    "    o.append(m)\n",
    "\n",
    "s = pd.DataFrame(o,columns=['azure'])\n",
    "\n",
    "google=[]\n",
    "with open('google_score.csv') as f :\n",
    "    reader = csv.reader(f)\n",
    "    for rows in reader:\n",
    "        google.append(rows)\n",
    "\n",
    "p = []\n",
    "for i in range(0,98):\n",
    "    m = google[1:100][i][1]\n",
    "    p.append(m)\n",
    "\n",
    "g = pd.DataFrame(p,columns=['google'])\n",
    "\n",
    "watson=[]\n",
    "with open('watson_score.csv') as f :\n",
    "    reader = csv.reader(f)\n",
    "    for rows in reader:\n",
    "        watson.append(rows)\n",
    "\n",
    "a = []\n",
    "for i in range(0,98):\n",
    "    m = watson[1:100][i][1]\n",
    "    a.append(m)\n",
    "\n",
    "h = pd.DataFrame(a,columns=['watson'])\n",
    "\n",
    "d = q.join(e)\n",
    "d = d.join(s)\n",
    "d = d.join(g)\n",
    "d = d.join(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manual</th>\n",
       "      <th>aws</th>\n",
       "      <th>azure</th>\n",
       "      <th>google</th>\n",
       "      <th>watson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.20000000298023224</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.89</td>\n",
       "      <td>-0.10000000149011612</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.4000000059604645</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.30000001192092896</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6000000238418579</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.30000001192092896</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.800000011920929</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.30000001192092896</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.699999988079071</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.30000001192092896</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4000000059604645</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4000000059604645</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20000000298023224</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.30000001192092896</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20000000298023224</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6000000238418579</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20000000298023224</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.30000001192092896</td>\n",
       "      <td>-0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.20000000298023224</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4000000059604645</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.10000000149011612</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.30000001192092896</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.4000000059604645</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-0.10000000149011612</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.800000011920929</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20000000298023224</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.10000000149011612</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.800000011920929</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.10000000149011612</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.30000001192092896</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.30000001192092896</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4000000059604645</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6000000238418579</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20000000298023224</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4000000059604645</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20000000298023224</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.20000000298023224</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8999999761581421</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.30000001192092896</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.20000000298023224</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4000000059604645</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.20000000298023224</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.20000000298023224</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   manual   aws azure                google watson\n",
       "0       0  0.99  0.5                    0.0    0.0\n",
       "1       1  0.88  0.5                    0.0   0.77\n",
       "2       0  0.85  0.5   -0.20000000298023224   0.71\n",
       "3       0  0.87  0.5                    0.0    0.0\n",
       "4       1  0.96  0.89  -0.10000000149011612   0.76\n",
       "5       1  0.69  0.87    0.4000000059604645   0.47\n",
       "6      -1  0.38  0.77   0.30000001192092896   0.63\n",
       "7       0  0.99  0.5     0.6000000238418579    0.0\n",
       "8       1  0.92  0.85                   0.5    0.6\n",
       "9       1  0.53   0.9   0.30000001192092896   0.79\n",
       "10      1  0.73   0.5     0.800000011920929   0.86\n",
       "11      1  0.41   0.5                   0.0   0.53\n",
       "12      1  0.68   0.5   0.30000001192092896   0.48\n",
       "13      1  0.67   0.5                   0.0   0.39\n",
       "14      1  0.64   0.5                   0.0   0.67\n",
       "15      0  0.28   0.2                   0.5  -0.76\n",
       "16      0  0.53   0.5                   0.0   0.48\n",
       "17      1  0.69   0.9     0.699999988079071   0.99\n",
       "18      1   0.7   0.9   0.30000001192092896   0.58\n",
       "19      1  0.96   0.9    0.4000000059604645    0.9\n",
       "20      1  0.51   0.8    0.4000000059604645   0.65\n",
       "21      1   0.7   0.8                   0.5   0.78\n",
       "22      1  0.91   0.5   0.20000000298023224    0.6\n",
       "23      0  0.99   0.5   0.30000001192092896   0.59\n",
       "24      1  0.74   0.5   0.20000000298023224   0.41\n",
       "25      1  0.88   0.9    0.6000000238418579   0.98\n",
       "26     -1  0.94   0.5   0.20000000298023224   0.52\n",
       "27     -1   0.9   0.8   0.30000001192092896   -0.6\n",
       "28      1  0.89   0.9   0.20000000298023224   0.57\n",
       "29      1  0.94   0.5    0.4000000059604645   0.73\n",
       "..    ...   ...   ...                   ...    ...\n",
       "68      1  0.29   0.7   0.10000000149011612   0.81\n",
       "69      1   0.8   0.7   0.30000001192092896   0.79\n",
       "70      1  0.89   0.8                   0.0   0.89\n",
       "71      1  0.93   0.5   -0.4000000059604645   0.22\n",
       "72      0  0.96   0.9  -0.10000000149011612   0.36\n",
       "73      1  0.73   0.8                   0.0   0.92\n",
       "74      1  0.52   0.9                  -0.5   0.92\n",
       "75      1  0.84   0.8     0.800000011920929   0.85\n",
       "76      1  0.64   0.5   0.20000000298023224   0.49\n",
       "77      1  0.45   0.2   0.10000000149011612    0.8\n",
       "78      1  0.98   0.7     0.800000011920929   0.59\n",
       "79      0  0.78   0.9   0.10000000149011612   0.85\n",
       "80      1  0.74   0.8   0.30000001192092896   0.57\n",
       "81      1  0.97   0.9   0.30000001192092896   0.59\n",
       "82      0  0.97   0.9                   0.0   0.75\n",
       "83      1  0.95   0.9    0.4000000059604645    0.9\n",
       "84      0  0.98   0.9    0.6000000238418579   0.83\n",
       "85      0  0.83   0.8                   0.0  -0.62\n",
       "86      1  0.73   0.5   0.20000000298023224   0.73\n",
       "87      0   0.7   0.9    0.4000000059604645   0.95\n",
       "88      0  0.94   0.5   0.20000000298023224   0.43\n",
       "89      0  0.52   0.5   0.20000000298023224   0.67\n",
       "90      1  0.73   0.9    0.8999999761581421   0.63\n",
       "91      0  0.83   0.8   0.30000001192092896   0.65\n",
       "92      0   0.9   0.5                   0.0    0.0\n",
       "93      0  0.95   0.9   0.20000000298023224   0.91\n",
       "94     -1   0.5   0.5                   0.0   0.58\n",
       "95      1  0.65   0.9    0.4000000059604645   0.58\n",
       "96      0  0.98   0.9   0.20000000298023224   0.78\n",
       "97      1  0.97   0.9   0.20000000298023224   0.95\n",
       "\n",
       "[98 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: || 100%\n"
     ]
    }
   ],
   "source": [
    "d = h2o.H2OFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aws': 'real',\n",
       " 'azure': 'enum',\n",
       " 'google': 'real',\n",
       " 'manual': 'int',\n",
       " 'watson': 'real'}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:98\n",
      "Cols:5\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>       </th><th>manual            </th><th>aws                </th><th>azure  </th><th>google              </th><th>watson             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>type   </td><td>int               </td><td>real               </td><td>enum   </td><td>real                </td><td>real               </td></tr>\n",
       "<tr><td>mins   </td><td>-1.0              </td><td>0.25               </td><td>       </td><td>-0.5                </td><td>-0.9               </td></tr>\n",
       "<tr><td>mean   </td><td>0.4897959183673469</td><td>0.762142857142857  </td><td>       </td><td>0.22857143212946088 </td><td>0.47959183673469385</td></tr>\n",
       "<tr><td>maxs   </td><td>1.0               </td><td>0.99               </td><td>       </td><td>0.8999999761581421  </td><td>0.99               </td></tr>\n",
       "<tr><td>sigma  </td><td>0.7214661183889154</td><td>0.17894449983338007</td><td>       </td><td>0.2632078106519106  </td><td>0.4493980357899323 </td></tr>\n",
       "<tr><td>zeros  </td><td>24                </td><td>0                  </td><td>       </td><td>21                  </td><td>4                  </td></tr>\n",
       "<tr><td>missing</td><td>0                 </td><td>0                  </td><td>0      </td><td>0                   </td><td>0                  </td></tr>\n",
       "<tr><td>0      </td><td>0.0               </td><td>0.99               </td><td>0.5    </td><td>0.0                 </td><td>0.0                </td></tr>\n",
       "<tr><td>1      </td><td>1.0               </td><td>0.88               </td><td>0.5    </td><td>0.0                 </td><td>0.77               </td></tr>\n",
       "<tr><td>2      </td><td>0.0               </td><td>0.85               </td><td>0.5    </td><td>-0.20000000298023224</td><td>0.71               </td></tr>\n",
       "<tr><td>3      </td><td>0.0               </td><td>0.87               </td><td>0.5    </td><td>0.0                 </td><td>0.0                </td></tr>\n",
       "<tr><td>4      </td><td>1.0               </td><td>0.96               </td><td>0.89   </td><td>-0.10000000149011612</td><td>0.76               </td></tr>\n",
       "<tr><td>5      </td><td>1.0               </td><td>0.69               </td><td>0.87   </td><td>0.4000000059604645  </td><td>0.47               </td></tr>\n",
       "<tr><td>6      </td><td>-1.0              </td><td>0.38               </td><td>0.77   </td><td>0.30000001192092896 </td><td>0.63               </td></tr>\n",
       "<tr><td>7      </td><td>0.0               </td><td>0.99               </td><td>0.5    </td><td>0.6000000238418579  </td><td>0.0                </td></tr>\n",
       "<tr><td>8      </td><td>1.0               </td><td>0.92               </td><td>0.85   </td><td>0.5                 </td><td>0.6                </td></tr>\n",
       "<tr><td>9      </td><td>1.0               </td><td>0.53               </td><td>0.9    </td><td>0.30000001192092896 </td><td>0.79               </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d['manual'] = d['manual'].asfactor()\n",
    "y = 'manual'\n",
    "x = list(d.columns)\n",
    "x.remove(y)\n",
    "train, test = d.split_frame([0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplearning Model Build progress: || 100%\n"
     ]
    }
   ],
   "source": [
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n",
    "#from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "dle= H2ODeepLearningEstimator()\n",
    "#gbm = H2OGradientBoostingEstimator()\n",
    "dle.train(x = x, y =y, training_frame = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsMultinomial: deeplearning\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.3859645821333194\n",
      "RMSE: 0.6212604784897551\n",
      "LogLoss: 1.5140004282686859\n",
      "Mean Per-Class Error: 0.5353535353535354\n",
      "Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>-1</b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>2.0</td>\n",
       "<td>3.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.6666667</td>\n",
       "<td>4 / 6</td></tr>\n",
       "<tr><td>1.0</td>\n",
       "<td>3.0</td>\n",
       "<td>5.0</td>\n",
       "<td>0.6666667</td>\n",
       "<td>6 / 9</td></tr>\n",
       "<tr><td>1.0</td>\n",
       "<td>5.0</td>\n",
       "<td>16.0</td>\n",
       "<td>0.2727273</td>\n",
       "<td>6 / 22</td></tr>\n",
       "<tr><td>4.0</td>\n",
       "<td>11.0</td>\n",
       "<td>22.0</td>\n",
       "<td>0.4324324</td>\n",
       "<td>16 / 37</td></tr></table></div>"
      ],
      "text/plain": [
       "-1    0    1    Error     Rate\n",
       "----  ---  ---  --------  -------\n",
       "2     3    1    0.666667  4 / 6\n",
       "1     3    5    0.666667  6 / 9\n",
       "1     5    16   0.272727  6 / 22\n",
       "4     11   22   0.432432  16 / 37"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-3 Hit Ratios: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>k</b></td>\n",
       "<td><b>hit_ratio</b></td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>0.5675676</td></tr>\n",
       "<tr><td>2</td>\n",
       "<td>0.8108108</td></tr>\n",
       "<tr><td>3</td>\n",
       "<td>1.0</td></tr></table></div>"
      ],
      "text/plain": [
       "k    hit_ratio\n",
       "---  -----------\n",
       "1    0.567568\n",
       "2    0.810811\n",
       "3    1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dle.model_performance(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm Model Build progress: || 100%\n"
     ]
    }
   ],
   "source": [
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "gbm = H2OGradientBoostingEstimator()\n",
    "gbm.train(x,y,train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>-1</b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>2.0</td>\n",
       "<td>3.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.6666667</td>\n",
       "<td>4 / 6</td></tr>\n",
       "<tr><td>1.0</td>\n",
       "<td>3.0</td>\n",
       "<td>5.0</td>\n",
       "<td>0.6666667</td>\n",
       "<td>6 / 9</td></tr>\n",
       "<tr><td>1.0</td>\n",
       "<td>5.0</td>\n",
       "<td>16.0</td>\n",
       "<td>0.2727273</td>\n",
       "<td>6 / 22</td></tr>\n",
       "<tr><td>4.0</td>\n",
       "<td>11.0</td>\n",
       "<td>22.0</td>\n",
       "<td>0.4324324</td>\n",
       "<td>16 / 37</td></tr></table></div>"
      ],
      "text/plain": [
       "-1    0    1    Error     Rate\n",
       "----  ---  ---  --------  -------\n",
       "2     3    1    0.666667  4 / 6\n",
       "1     3    5    0.666667  6 / 9\n",
       "1     5    16   0.272727  6 / 22\n",
       "4     11   22   0.432432  16 / 37"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dle.confusion_matrix(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>-1</b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>2.0</td>\n",
       "<td>2.0</td>\n",
       "<td>3.0</td>\n",
       "<td>0.7142857</td>\n",
       "<td>5 / 7</td></tr>\n",
       "<tr><td>2.0</td>\n",
       "<td>11.0</td>\n",
       "<td>2.0</td>\n",
       "<td>0.2666667</td>\n",
       "<td>4 / 15</td></tr>\n",
       "<tr><td>2.0</td>\n",
       "<td>8.0</td>\n",
       "<td>29.0</td>\n",
       "<td>0.2564103</td>\n",
       "<td>10 / 39</td></tr>\n",
       "<tr><td>6.0</td>\n",
       "<td>21.0</td>\n",
       "<td>34.0</td>\n",
       "<td>0.3114754</td>\n",
       "<td>19 / 61</td></tr></table></div>"
      ],
      "text/plain": [
       "-1    0    1    Error     Rate\n",
       "----  ---  ---  --------  -------\n",
       "2     2    3    0.714286  5 / 7\n",
       "2     11   2    0.266667  4 / 15\n",
       "2     8    29   0.25641   10 / 39\n",
       "6     21   34   0.311475  19 / 61"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dle.confusion_matrix(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>-1</b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0.0</td>\n",
       "<td>3.0</td>\n",
       "<td>2.0</td>\n",
       "<td>1.0</td>\n",
       "<td>5 / 5</td></tr>\n",
       "<tr><td>0.0</td>\n",
       "<td>3.0</td>\n",
       "<td>5.0</td>\n",
       "<td>0.625</td>\n",
       "<td>5 / 8</td></tr>\n",
       "<tr><td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>11.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0 / 11</td></tr>\n",
       "<tr><td>0.0</td>\n",
       "<td>6.0</td>\n",
       "<td>18.0</td>\n",
       "<td>0.4166667</td>\n",
       "<td>10 / 24</td></tr></table></div>"
      ],
      "text/plain": [
       "-1    0    1    Error     Rate\n",
       "----  ---  ---  --------  -------\n",
       "0     3    2    1         5 / 5\n",
       "0     3    5    0.625     5 / 8\n",
       "0     0    11   0         0 / 11\n",
       "0     6    18   0.416667  10 / 24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.confusion_matrix(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
